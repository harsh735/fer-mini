{"ast":null,"code":"import * as faceapi from \"face-api.js\";\nimport Jimp from \"jimp\";\nimport * as tf from \"@tensorflow/tfjs\";\nexport const loadModels = () => {\n  const MODEL_URL = `${process.env.PUBLIC_URL}/models`;\n  return Promise.all([faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL)]).then(() => {\n    console.log(\"loaded\");\n  });\n};\nexport const detectFaces = async image => {\n  if (!image) {\n    return;\n  }\n\n  const imgSize = image.getBoundingClientRect();\n  const displaySize = {\n    width: imgSize.width,\n    height: imgSize.height\n  };\n\n  if (displaySize.height === 0) {\n    return;\n  }\n\n  const faces = await faceapi.detectAllFaces(image, new faceapi.TinyFaceDetectorOptions({\n    inputSize: 320\n  }));\n  return faceapi.resizeResults(faces, displaySize);\n};\nvar result;\nexport const drawResults = async (image, canvas, results, type, tocrop, facecanvas, model) => {\n  if (image && canvas && results) {\n    const imgSize = image.getBoundingClientRect();\n    const displaySize = {\n      width: imgSize.width,\n      height: imgSize.height\n    };\n    faceapi.matchDimensions(canvas, displaySize);\n    canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\n    var resizedDetections;\n\n    if (displaySize.width !== 0) {\n      resizedDetections = faceapi.resizeResults(results, displaySize);\n\n      if (resizedDetections[0]) {\n        const box = resizedDetections[0].box;\n        const drawOptions = {\n          lineWidth: 2\n        };\n        const drawBox = new faceapi.draw.DrawBox(box, drawOptions);\n        drawBox.draw(canvas);\n        Jimp.read(tocrop).then(image => {\n          image.crop(resizedDetections[0].box.x, resizedDetections[0].box.y, resizedDetections[0].box.width, resizedDetections[0].box.height);\n          image.resize(48, 48);\n          image = image.bitmap.data;\n          image = tf.tensor3d(image, [48, 48, 4]);\n          image = image.mean(2);\n          image = image.expandDims(2);\n          image = image.div(255);\n          image = image.reshape([1, 48, 48, 1]);\n          var output = model.predict(image);\n          output = output.dataSync();\n          result = output;\n        });\n      }\n\n      return result;\n    }\n  }\n};","map":{"version":3,"sources":["/home/harsh/Desktop/cs230-fer-master/webapp/src/Face API/faceApi.js"],"names":["faceapi","Jimp","tf","loadModels","MODEL_URL","process","env","PUBLIC_URL","Promise","all","nets","tinyFaceDetector","loadFromUri","then","console","log","detectFaces","image","imgSize","getBoundingClientRect","displaySize","width","height","faces","detectAllFaces","TinyFaceDetectorOptions","inputSize","resizeResults","result","drawResults","canvas","results","type","tocrop","facecanvas","model","matchDimensions","getContext","clearRect","resizedDetections","box","drawOptions","lineWidth","drawBox","draw","DrawBox","read","crop","x","y","resize","bitmap","data","tensor3d","mean","expandDims","div","reshape","output","predict","dataSync"],"mappings":"AAAA,OAAO,KAAKA,OAAZ,MAAyB,aAAzB;AACA,OAAOC,IAAP,MAAiB,MAAjB;AACA,OAAO,KAAKC,EAAZ,MAAoB,kBAApB;AAIA,OAAO,MAAMC,UAAU,GAAG,MAAM;AAC9B,QAAMC,SAAS,GAAI,GAAEC,OAAO,CAACC,GAAR,CAAYC,UAAW,SAA5C;AAEA,SAAOC,OAAO,CAACC,GAAR,CAAY,CACjBT,OAAO,CAACU,IAAR,CAAaC,gBAAb,CAA8BC,WAA9B,CAA0CR,SAA1C,CADiB,CAAZ,EAEJS,IAFI,CAEC,MAAI;AACVC,IAAAA,OAAO,CAACC,GAAR,CAAY,QAAZ;AACD,GAJM,CAAP;AAKD,CARM;AAUP,OAAO,MAAMC,WAAW,GAAG,MAAMC,KAAN,IAAe;AACxC,MAAI,CAACA,KAAL,EAAY;AACV;AACD;;AAED,QAAMC,OAAO,GAAGD,KAAK,CAACE,qBAAN,EAAhB;AACA,QAAMC,WAAW,GAAG;AAACC,IAAAA,KAAK,EAAEH,OAAO,CAACG,KAAhB;AAAuBC,IAAAA,MAAM,EAAEJ,OAAO,CAACI;AAAvC,GAApB;;AACA,MAAIF,WAAW,CAACE,MAAZ,KAAuB,CAA3B,EAA8B;AAC5B;AACD;;AAED,QAAMC,KAAK,GAAG,MAAMvB,OAAO,CACxBwB,cADiB,CAEhBP,KAFgB,EAGhB,IAAIjB,OAAO,CAACyB,uBAAZ,CAAoC;AAACC,IAAAA,SAAS,EAAE;AAAZ,GAApC,CAHgB,CAApB;AAMA,SAAO1B,OAAO,CAAC2B,aAAR,CAAsBJ,KAAtB,EAA6BH,WAA7B,CAAP;AACD,CAlBM;AAoBP,IAAIQ,MAAJ;AAEA,OAAO,MAAMC,WAAW,GAAG,OAAOZ,KAAP,EAAca,MAAd,EAAsBC,OAAtB,EAA+BC,IAA/B,EAAoCC,MAApC,EAA2CC,UAA3C,EAAsDC,KAAtD,KAAgE;AACzF,MAAIlB,KAAK,IAAIa,MAAT,IAAmBC,OAAvB,EAAgC;AAC9B,UAAMb,OAAO,GAAGD,KAAK,CAACE,qBAAN,EAAhB;AACA,UAAMC,WAAW,GAAG;AAACC,MAAAA,KAAK,EAAEH,OAAO,CAACG,KAAhB;AAAuBC,MAAAA,MAAM,EAAEJ,OAAO,CAACI;AAAvC,KAApB;AACAtB,IAAAA,OAAO,CAACoC,eAAR,CAAwBN,MAAxB,EAAgCV,WAAhC;AACAU,IAAAA,MAAM,CAACO,UAAP,CAAkB,IAAlB,EAAwBC,SAAxB,CAAkC,CAAlC,EAAqC,CAArC,EAAwCR,MAAM,CAACT,KAA/C,EAAsDS,MAAM,CAACR,MAA7D;AACA,QAAIiB,iBAAJ;;AACA,QAAGnB,WAAW,CAACC,KAAZ,KAAoB,CAAvB,EAAyB;AACzBkB,MAAAA,iBAAiB,GAAGvC,OAAO,CAAC2B,aAAR,CAAsBI,OAAtB,EAA+BX,WAA/B,CAApB;;AACA,UAAGmB,iBAAiB,CAAC,CAAD,CAApB,EAAwB;AACxB,cAAMC,GAAG,GAAED,iBAAiB,CAAC,CAAD,CAAjB,CAAqBC,GAAhC;AACA,cAAMC,WAAW,GAAG;AAClBC,UAAAA,SAAS,EAAE;AADO,SAApB;AAGA,cAAMC,OAAO,GAAG,IAAI3C,OAAO,CAAC4C,IAAR,CAAaC,OAAjB,CAAyBL,GAAzB,EAA8BC,WAA9B,CAAhB;AACAE,QAAAA,OAAO,CAACC,IAAR,CAAad,MAAb;AAGJ7B,QAAAA,IAAI,CAAC6C,IAAL,CAAUb,MAAV,EACEpB,IADF,CACOI,KAAK,IAAI;AACbA,UAAAA,KAAK,CAAC8B,IAAN,CAAYR,iBAAiB,CAAC,CAAD,CAAjB,CAAqBC,GAArB,CAAyBQ,CAArC,EAAwCT,iBAAiB,CAAC,CAAD,CAAjB,CAAqBC,GAArB,CAAyBS,CAAjE,EAAoEV,iBAAiB,CAAC,CAAD,CAAjB,CAAqBC,GAArB,CAAyBnB,KAA7F,EAAmGkB,iBAAiB,CAAC,CAAD,CAAjB,CAAqBC,GAArB,CAAyBlB,MAA5H;AACAL,UAAAA,KAAK,CAACiC,MAAN,CAAa,EAAb,EAAgB,EAAhB;AACHjC,UAAAA,KAAK,GAACA,KAAK,CAACkC,MAAN,CAAaC,IAAnB;AACAnC,UAAAA,KAAK,GAAGf,EAAE,CAACmD,QAAH,CAAYpC,KAAZ,EAAkB,CAAC,EAAD,EAAI,EAAJ,EAAO,CAAP,CAAlB,CAAR;AACAA,UAAAA,KAAK,GAAGA,KAAK,CAACqC,IAAN,CAAW,CAAX,CAAR;AACArC,UAAAA,KAAK,GAAGA,KAAK,CAACsC,UAAN,CAAiB,CAAjB,CAAR;AACAtC,UAAAA,KAAK,GAAGA,KAAK,CAACuC,GAAN,CAAU,GAAV,CAAR;AACAvC,UAAAA,KAAK,GAAGA,KAAK,CAACwC,OAAN,CAAc,CAAC,CAAD,EAAG,EAAH,EAAM,EAAN,EAAS,CAAT,CAAd,CAAR;AACA,cAAIC,MAAM,GAAGvB,KAAK,CAACwB,OAAN,CAAc1C,KAAd,CAAb;AACAyC,UAAAA,MAAM,GAAGA,MAAM,CAACE,QAAP,EAAT;AACAhC,UAAAA,MAAM,GAAG8B,MAAT;AACC,SAbD;AAcC;;AACC,aAAO9B,MAAP;AACK;AAEJ;AAEF,CAtCM","sourcesContent":["import * as faceapi from \"face-api.js\";\nimport Jimp from \"jimp\";\nimport * as tf from \"@tensorflow/tfjs\"\n\n\n\nexport const loadModels = () => {\n  const MODEL_URL = `${process.env.PUBLIC_URL}/models`;\n\n  return Promise.all([\n    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n  ]).then(()=>{\n    console.log(\"loaded\")\n  });\n};\n\nexport const detectFaces = async image => {\n  if (!image) {\n    return;\n  }\n\n  const imgSize = image.getBoundingClientRect();\n  const displaySize = {width: imgSize.width, height: imgSize.height};\n  if (displaySize.height === 0) {\n    return;\n  }\n\n  const faces = await faceapi\n    .detectAllFaces(\n      image,\n      new faceapi.TinyFaceDetectorOptions({inputSize: 320})\n    )\n\n  return faceapi.resizeResults(faces, displaySize);\n};\n\nvar result;\n\nexport const drawResults = async (image, canvas, results, type,tocrop,facecanvas,model) => {\n  if (image && canvas && results) {\n    const imgSize = image.getBoundingClientRect();\n    const displaySize = {width: imgSize.width, height: imgSize.height};\n    faceapi.matchDimensions(canvas, displaySize);\n    canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\n    var resizedDetections;\n    if(displaySize.width!==0){\n    resizedDetections = faceapi.resizeResults(results, displaySize);  \n    if(resizedDetections[0]){\n    const box =resizedDetections[0].box\n    const drawOptions = {\n      lineWidth: 2\n    }\n    const drawBox = new faceapi.draw.DrawBox(box, drawOptions)\n    drawBox.draw(canvas)\n  \n\nJimp.read(tocrop)\n .then(image => {\n   image.crop( resizedDetections[0].box.x, resizedDetections[0].box.y, resizedDetections[0].box.width,resizedDetections[0].box.height ); \n   image.resize(48,48)\nimage=image.bitmap.data\nimage = tf.tensor3d(image,[48,48,4])\nimage = image.mean(2)\nimage = image.expandDims(2)\nimage = image.div(255)\nimage = image.reshape([1,48,48,1])\nvar output = model.predict(image)\noutput = output.dataSync()\nresult = output\n})\n}\n  return result \n      }\n    \n  }\n  \n};\n"]},"metadata":{},"sourceType":"module"}